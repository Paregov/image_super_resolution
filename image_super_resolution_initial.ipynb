{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing an image super resolution.\n",
    "## Here I'm going to do some quick training as proof of concept (PoC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from models import generator_no_residual, generator_with_residual, discriminator\n",
    "from utils import check_path_exists\n",
    "from data_loader import load_images_with_truth\n",
    "from loss_functions import perceptual_loss, perceptual_loss_16, perceptual_loss_19\n",
    "from loss_functions import texture_loss_multi_layers, perceptual_plus_texture_loss, perceptual_16_plus_texture_loss\n",
    "from visualizations import plot_images_for_compare, plot_images_for_compare_separate, compare_models, compare_models_single_image\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable this flag if running on the laptop with smaller GPU.\n",
    "running_on_laptop = False\n",
    "\n",
    "# This will use the smaller datasets (train_small, val_small, test_small).\n",
    "use_small_dataset = True\n",
    "\n",
    "# Set to true if you want to use CelebA dataset. Otherwise it will use MS COCO.\n",
    "use_dataset_celeba = False\n",
    "\n",
    "# Set to true and it will not execute training. Usefull when just want to plot the results.\n",
    "disable_training = False\n",
    "\n",
    "# What to be the verbose level during training.\n",
    "training_verbose = 2\n",
    "\n",
    "enable_p = True\n",
    "enable_p16 = True\n",
    "enable_p19 = True\n",
    "enable_t = True\n",
    "enable_pt = True\n",
    "enable_pt16 = True\n",
    "enable_pt16_bci = True\n",
    "enable_pt_bci = True\n",
    "enable_pt16_no_res = True\n",
    "\n",
    "train_epochs = 100\n",
    "train_batch_size = 32\n",
    "test_image_index_to_show = range(20)\n",
    "optimizer = Adam(lr=0.0001)\n",
    "\n",
    "if running_on_laptop:\n",
    "    train_epochs = 100\n",
    "    train_batch_size = 8\n",
    "    test_image_index_to_show = range(20)\n",
    "    optimizer = Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/MSCOCO/train/*\n",
      "./data/MSCOCO/val/*\n",
      "./data/MSCOCO/test/*\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset path\n",
    "dataset = \"MSCOCO\"\n",
    "if use_dataset_celeba:\n",
    "    dataset = \"celeba\"\n",
    "\n",
    "if not use_small_dataset:\n",
    "    train_dataset_path = './data/{0}/train/*'.format(dataset)\n",
    "    validation_dataset_path = './data/{0}/val/*'.format(dataset)\n",
    "    test_dataset_path = './data/{0}/test/*'.format(dataset)\n",
    "else:\n",
    "    train_dataset_path = './data/{0}/train_small/*'.format(dataset)\n",
    "    validation_dataset_path = './data/{0}/val_small/*'.format(dataset)\n",
    "    test_dataset_path = './data/{0}/test_small/*'.format(dataset)\n",
    "\n",
    "print(train_dataset_path)\n",
    "print(validation_dataset_path)\n",
    "print(test_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptual_loss_checkpint_path = './saved_models/weights.best.train.mscoco.pl.hdf5'\n",
    "perceptual_loss_16_checkpoint_path = './saved_models/weights.best.train.mscoco.pl16.hdf5'\n",
    "perceptual_loss_19_checkpoint_path = './saved_models/weights.best.train.mscoco.pl19.hdf5'\n",
    "texture_loss_ml_checkpoint_path = './saved_models/weights.best.train.mscoco.tl_ml.hdf5'\n",
    "texture_plus_perceptual_loss_checkpoint_path = './saved_models/weights.best.train.mscoco.tl_plus_pl.hdf5'\n",
    "perceptual_16_plus_texture_loss_checkpoint_path = './saved_models/weights.best.train.mscoco.pl16_plus_tl.hdf5'\n",
    "perceptual_16_plus_texture_loss_bci_checkpoint_path = './saved_models/weights.best.train.mscoco.pl16_plus_tl_bci.hdf5'\n",
    "perceptual_plus_texture_loss_bci_checkpoint_path = './saved_models/weights.best.train.mscoco.pl_plus_tl_bci.hdf5'\n",
    "perceptual_16_plus_texture_loss_no_res_checkpoint_path = './saved_models/weights.best.train.mscoco.pl16_plus_tl_no_res.hdf5'\n",
    "\n",
    "if use_dataset_celeba:\n",
    "    perceptual_loss_checkpint_path = './saved_models/weights.best.train.celeba.pl.hdf5'\n",
    "    perceptual_loss_16_checkpoint_path = './saved_models/weights.best.train.celeba.pl16.hdf5'\n",
    "    perceptual_loss_19_checkpoint_path = './saved_models/weights.best.train.celeba.pl19.hdf5'\n",
    "    texture_loss_ml_checkpoint_path = './saved_models/weights.best.train.celeba.tl_ml.hdf5'\n",
    "    texture_plus_perceptual_loss_checkpoint_path = './saved_models/weights.best.train.celeba.tl_plus_pl.hdf5'\n",
    "    perceptual_16_plus_texture_loss_checkpoint_path = './saved_models/weights.best.train.celeba.pl16_plus_tl.hdf5'\n",
    "    perceptual_16_plus_texture_loss_bci_checkpoint_path = './saved_models/weights.best.train.celeba.pl16_plus_tl_bci.hdf5'\n",
    "    perceptual_plus_texture_loss_bci_checkpoint_path = './saved_models/weights.best.train.celeba.pl_plus_tl_bci.hdf5'\n",
    "    perceptual_16_plus_texture_loss_no_res_checkpoint_path = './saved_models/weights.best.train.celeba.pl16_plus_tl_no_res.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Some of the parameters are used from the global space\n",
    "def model_train(model, optimizer, loss_function, checkpoint_path, verbose=2):\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
    "    checkpointer = ModelCheckpoint(filepath=checkpoint_path, \n",
    "                               verbose=verbose, save_best_only=True)\n",
    "    early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=verbose)\n",
    "\n",
    "    model.fit(train_data_tensors, train_truth_tensors,\n",
    "              validation_data=(validation_data_tensors, validation_truth_tensors),\n",
    "              epochs=train_epochs, batch_size=train_batch_size, callbacks=[checkpointer, early_stopper], verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Some of the parameters are used from the global space\n",
    "def model_predict(model, checkpoint_path):\n",
    "    return\n",
    "    model.load_weights(checkpoint_path)\n",
    "\n",
    "    print('Predicting...')\n",
    "    predictions = model.predict(test_data_tensors)\n",
    "\n",
    "    print('Plotting the results...')\n",
    "    plot_images_for_compare_separate(test_data_tensors, predictions, test_truth_tensors, test_image_index_to_show)\n",
    "\n",
    "    print('All done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Some of the parameters are used from the global space\n",
    "def model_predict_2(model, checkpoint_path):\n",
    "    model.load_weights(checkpoint_path)\n",
    "\n",
    "    print('Predicting...')\n",
    "    predictions = model.predict(test_data_tensors)\n",
    "\n",
    "    print('Plotting the results...')\n",
    "    plot_images_for_compare(test_data_tensors, predictions, test_truth_tensors, test_image_index_to_show)\n",
    "\n",
    "    print('All done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create models, train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_p(get_model_only=False, summary=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if not get_model_only:\n",
    "        model = generator_with_residual(input_shape=train_data.shape[1:], summary=False, add_bicubic=False)\n",
    "        if not disable_training:\n",
    "            model_train(model=model, optimizer=optimizer, loss_function=perceptual_loss, checkpoint_path=perceptual_loss_checkpint_path,\n",
    "                        verbose=training_verbose)\n",
    "        model_predict(model, perceptual_loss_checkpint_path)\n",
    "        # Free the memory\n",
    "        del model\n",
    "        gc.collect()\n",
    "    \n",
    "    # Recreate the model and return it\n",
    "    return generator_with_residual(input_shape=train_data.shape[1:], summary=False, add_bicubic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the functions to load the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we use all color images in MSCOCO [31]\n",
    "that have at least 384 pixels on the short side resulting in\n",
    "roughly 200k images. All images are cropped centrally to a\n",
    "square and then downsampled to 256×256 to reduce noise\n",
    "and JPEG artifacts. During training, we fix the size of the\n",
    "input ILR to 32×32. As the scale of objects in the MSCOCO\n",
    "dataset is too small when downsampled to such a small size,\n",
    "we downsample the 256×256 images by \u000b",
    " and then crop\n",
    "these to patches of size 32×32. After training the model\n",
    "for any given scaling factor \u000b",
    ", the input to the fully convolutional\n",
    "network at test time can be an image of arbitrary\n",
    "dimensions w×h which is then upscaled to (\u000b",
    "w)×(\u000b",
    "h)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 154168/154168 [34:40<00:00, 74.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 18056/18056 [04:05<00:00, 74.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 68.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images:  154168\n",
      "Validation images:  18056\n",
      "Test images:  20\n"
     ]
    }
   ],
   "source": [
    "print('Loading train data:')\n",
    "train_data, train_truth = load_images_with_truth(train_dataset_path, 4)\n",
    "print('Loading validation data:')\n",
    "validation_data, validation_truth = load_images_with_truth(validation_dataset_path, 4)\n",
    "print('Loading test data:')\n",
    "test_data, test_truth = load_images_with_truth(test_dataset_path, 4)\n",
    "\n",
    "print(\"Train images: \", len(train_data))\n",
    "print('Validation images: ', len(validation_data))\n",
    "print(\"Test images: \", len(test_data))\n",
    "\n",
    "train_data_tensors = train_data.astype('float32')/255\n",
    "train_truth_tensors = train_truth.astype('float32')/255\n",
    "\n",
    "validation_data_tensors = validation_data.astype('float32')/255\n",
    "validation_truth_tensors = validation_truth.astype('float32')/255\n",
    "\n",
    "test_data_tensors = test_data.astype('float32')/255\n",
    "test_truth_tensors = test_truth.astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to create separate models even though that they are based on the same architecture. This is because we want to have new default parameters each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptual loss based on VGG19 (selected layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 154168 samples, validate on 18056 samples\n",
      "Epoch 1/100\n",
      " - 1167s - loss: 0.1217 - acc: 0.7623 - val_loss: 0.1049 - val_acc: 0.7677\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.10491, saving model to ./saved_models/weights.best.train.mscoco.pl.hdf5\n",
      "Epoch 2/100\n",
      " - 1126s - loss: 0.1012 - acc: 0.7789 - val_loss: 0.0981 - val_acc: 0.7843\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.10491 to 0.09813, saving model to ./saved_models/weights.best.train.mscoco.pl.hdf5\n",
      "Epoch 3/100\n",
      " - 1091s - loss: 0.0960 - acc: 0.7806 - val_loss: 0.0939 - val_acc: 0.7807\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.09813 to 0.09392, saving model to ./saved_models/weights.best.train.mscoco.pl.hdf5\n",
      "Epoch 4/100\n",
      " - 1095s - loss: 0.0928 - acc: 0.7787 - val_loss: 0.0918 - val_acc: 0.7812\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.09392 to 0.09175, saving model to ./saved_models/weights.best.train.mscoco.pl.hdf5\n",
      "Epoch 5/100\n",
      " - 1075s - loss: 0.0906 - acc: 0.7748 - val_loss: 0.0898 - val_acc: 0.7856\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09175 to 0.08980, saving model to ./saved_models/weights.best.train.mscoco.pl.hdf5\n",
      "Epoch 6/100\n",
      " - 1096s - loss: 0.0891 - acc: 0.7705 - val_loss: 0.0884 - val_acc: 0.7593\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.08980 to 0.08835, saving model to ./saved_models/weights.best.train.mscoco.pl.hdf5\n",
      "Epoch 7/100\n",
      " - 1062s - loss: 0.0879 - acc: 0.7650 - val_loss: 0.0873 - val_acc: 0.7584\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.08835 to 0.08734, saving model to ./saved_models/weights.best.train.mscoco.pl.hdf5\n",
      "Epoch 8/100\n",
      " - 1054s - loss: 0.0869 - acc: 0.7591 - val_loss: 0.0868 - val_acc: 0.7440\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.08734 to 0.08676, saving model to ./saved_models/weights.best.train.mscoco.pl.hdf5\n",
      "Epoch 9/100\n",
      " - 1106s - loss: 0.0861 - acc: 0.7515 - val_loss: 0.0861 - val_acc: 0.7549\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.08676 to 0.08607, saving model to ./saved_models/weights.best.train.mscoco.pl.hdf5\n",
      "Epoch 10/100\n",
      " - 1060s - loss: 0.0855 - acc: 0.7436 - val_loss: 0.0858 - val_acc: 0.7521\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.08607 to 0.08580, saving model to ./saved_models/weights.best.train.mscoco.pl.hdf5\n",
      "Epoch 11/100\n",
      " - 1062s - loss: 0.0849 - acc: 0.7354 - val_loss: 0.0850 - val_acc: 0.7310\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.08580 to 0.08498, saving model to ./saved_models/weights.best.train.mscoco.pl.hdf5\n",
      "Epoch 12/100\n",
      " - 1077s - loss: 0.0844 - acc: 0.7274 - val_loss: 0.0844 - val_acc: 0.7071\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.08498 to 0.08445, saving model to ./saved_models/weights.best.train.mscoco.pl.hdf5\n",
      "Epoch 13/100\n",
      " - 1058s - loss: 0.0839 - acc: 0.7191 - val_loss: 0.0844 - val_acc: 0.7059\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.08445 to 0.08438, saving model to ./saved_models/weights.best.train.mscoco.pl.hdf5\n",
      "Epoch 14/100\n",
      " - 1102s - loss: 0.0835 - acc: 0.7107 - val_loss: 0.0837 - val_acc: 0.6929\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.08438 to 0.08368, saving model to ./saved_models/weights.best.train.mscoco.pl.hdf5\n",
      "Epoch 15/100\n",
      " - 1041s - loss: 0.0832 - acc: 0.7026 - val_loss: 0.0833 - val_acc: 0.7013\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.08368 to 0.08329, saving model to ./saved_models/weights.best.train.mscoco.pl.hdf5\n",
      "Epoch 16/100\n"
     ]
    }
   ],
   "source": [
    "if enable_p:\n",
    "    model = generator_with_residual(input_shape=train_data.shape[1:], summary=False, add_bicubic=False)\n",
    "    if not disable_training:\n",
    "        model_train(model=model, optimizer=optimizer, loss_function=perceptual_loss, checkpoint_path=perceptual_loss_checkpint_path, verbose=training_verbose)\n",
    "    model_predict(model, perceptual_loss_checkpint_path)\n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peceptual loss based on VGG19 (block3_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_p19:\n",
    "    model19 = generator_with_residual(input_shape=train_data.shape[1:], summary=False, add_bicubic=False)\n",
    "    if not disable_training:\n",
    "        model_train(model=model19, optimizer=optimizer, loss_function=perceptual_loss_19, checkpoint_path=perceptual_loss_19_checkpoint_path)\n",
    "    model_predict(model=model19, checkpoint_path=perceptual_loss_19_checkpoint_path)\n",
    "    del model19\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peceptual loss based on VGG16 (block3_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_p16:\n",
    "    model16 = generator_with_residual(input_shape=train_data.shape[1:], summary=False, add_bicubic=False)\n",
    "    if not disable_training:\n",
    "        model_train(model=model16, optimizer=optimizer, loss_function=perceptual_loss_16, checkpoint_path=perceptual_loss_16_checkpoint_path, verbose=training_verbose)\n",
    "    model_predict(model=model16, checkpoint_path=perceptual_loss_16_checkpoint_path)\n",
    "    del model16\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texture loss (multi layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_t:\n",
    "    model_tl_ml = generator_with_residual(input_shape=train_data.shape[1:], summary=False, add_bicubic=False)\n",
    "    if not disable_training:\n",
    "        model_train(model=model_tl_ml, optimizer=optimizer, loss_function=texture_loss_multi_layers, checkpoint_path=texture_loss_ml_checkpoint_path, verbose=training_verbose)\n",
    "    model_predict(model=model_tl_ml, checkpoint_path=texture_loss_ml_checkpoint_path)\n",
    "    del model_tl_ml\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texture loss plus perceptual loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_pt:\n",
    "    model_tl_plus_pl = generator_with_residual(input_shape=train_data.shape[1:], summary=False, add_bicubic=False)\n",
    "    if not disable_training:\n",
    "        model_train(model=model_tl_plus_pl, optimizer=optimizer, loss_function=perceptual_plus_texture_loss,\n",
    "                    checkpoint_path=texture_plus_perceptual_loss_checkpoint_path, verbose=training_verbose)\n",
    "    model_predict(model=model_tl_plus_pl, checkpoint_path=texture_plus_perceptual_loss_checkpoint_path)\n",
    "    del model_tl_plus_pl\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peceptual loss 16 + texture loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_pt16:\n",
    "    model_pl_16_plus_tl = generator_with_residual(input_shape=train_data.shape[1:], summary=False, add_bicubic=False)\n",
    "    if not disable_training:\n",
    "        model_train(model=model_pl_16_plus_tl, optimizer=optimizer, loss_function=perceptual_16_plus_texture_loss,\n",
    "                    checkpoint_path=perceptual_16_plus_texture_loss_checkpoint_path, verbose=training_verbose)\n",
    "    model_predict(model=model_pl_16_plus_tl, checkpoint_path=perceptual_16_plus_texture_loss_checkpoint_path)\n",
    "    del model_pl_16_plus_tl\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try some with bicubic interpolation included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peceptual loss 16 + texture loss. Bicubic interpolation included in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_pt16_bci:\n",
    "    model_pl_16_plus_tl_bci = generator_with_residual(input_shape=train_data.shape[1:], summary=False, add_bicubic=True)\n",
    "    if not disable_training:\n",
    "        model_train(model=model_pl_16_plus_tl_bci, optimizer=optimizer, loss_function=perceptual_16_plus_texture_loss,\n",
    "                    checkpoint_path=perceptual_16_plus_texture_loss_bci_checkpoint_path, verbose=training_verbose)\n",
    "    model_predict(model=model_pl_16_plus_tl_bci, checkpoint_path=perceptual_16_plus_texture_loss_bci_checkpoint_path)\n",
    "    del model_pl_16_plus_tl_bci\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptual plus texture loss. Bicubic interpolation included in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_pt_bci:\n",
    "    model_pl_plus_tl_bci = generator_with_residual(input_shape=train_data.shape[1:], summary=False, add_bicubic=True)\n",
    "    if not disable_training:\n",
    "        model_train(model=model_pl_plus_tl_bci, optimizer=optimizer, loss_function=perceptual_plus_texture_loss,\n",
    "                    checkpoint_path=perceptual_plus_texture_loss_bci_checkpoint_path, verbose=training_verbose)\n",
    "    model_predict(model=model_pl_plus_tl_bci, checkpoint_path=perceptual_plus_texture_loss_bci_checkpoint_path)\n",
    "    del model_pl_plus_tl_bci\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peceptual loss 16 + texture loss. No residual network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enable_pt16_no_res:\n",
    "    model_pl_16_plus_tl_no_res = generator_no_residual(input_shape=train_data.shape[1:], summary=False)\n",
    "    if not disable_training:\n",
    "        model_train(model=model_pl_16_plus_tl_no_res, optimizer=optimizer, loss_function=perceptual_16_plus_texture_loss,\n",
    "                    checkpoint_path=perceptual_16_plus_texture_loss_no_res_checkpoint_path, verbose=training_verbose)\n",
    "    model_predict(model=model_pl_16_plus_tl_no_res, checkpoint_path=perceptual_16_plus_texture_loss_no_res_checkpoint_path)\n",
    "    del model_pl_16_plus_tl_no_res\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare all of the models at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_data = []\n",
    "\n",
    "if enable_p:\n",
    "    model = generator_with_residual(input_shape=train_data.shape[1:], summary=False, add_bicubic=False)\n",
    "    models_data.append({'name': \"P\", 'model': model, 'checkpoint': perceptual_loss_checkpint_path})\n",
    "\n",
    "if enable_p16:\n",
    "    model19 = generator_with_residual(input_shape=train_data.shape[1:], summary=False, add_bicubic=False)\n",
    "    models_data.append({'name': \"P VGG19\", 'model': model19, 'checkpoint': perceptual_loss_19_checkpoint_path})\n",
    "\n",
    "if enable_p19:\n",
    "    model16 = generator_with_residual(input_shape=train_data.shape[1:], summary=False, add_bicubic=False)\n",
    "    models_data.append({'name': \"P VGG16\", 'model': model16, 'checkpoint': perceptual_loss_16_checkpoint_path})\n",
    "\n",
    "if enable_t:\n",
    "    model_tl_ml = generator_with_residual(input_shape=train_data.shape[1:], summary=False, add_bicubic=False)\n",
    "    models_data.append({'name': \"T\", 'model': model_tl_ml, 'checkpoint': texture_loss_ml_checkpoint_path})\n",
    "\n",
    "if enable_pt:\n",
    "    model_tl_plus_pl = generator_with_residual(input_shape=train_data.shape[1:], summary=False, add_bicubic=False)\n",
    "    models_data.append({'name': \"PT VGG19\", 'model': model_tl_plus_pl, 'checkpoint': texture_plus_perceptual_loss_checkpoint_path})\n",
    "\n",
    "if enable_pt16:\n",
    "    model_pl_16_plus_tl = generator_with_residual(input_shape=train_data.shape[1:], summary=False, add_bicubic=False)\n",
    "    models_data.append({'name': \"PT VGG16\", 'model': model_pl_16_plus_tl, 'checkpoint': perceptual_16_plus_texture_loss_checkpoint_path})\n",
    "\n",
    "if enable_pt16_bci:\n",
    "    model_pl_16_plus_tl_bci = generator_with_residual(input_shape=train_data.shape[1:], summary=False, add_bicubic=True)\n",
    "    models_data.append({'name': \"PT VGG16 (BCI)\", 'model': model_pl_16_plus_tl_bci, 'checkpoint': perceptual_16_plus_texture_loss_bci_checkpoint_path})\n",
    "\n",
    "if enable_pt_bci:\n",
    "    model_pl_plus_tl_bci = generator_with_residual(input_shape=train_data.shape[1:], summary=False, add_bicubic=True)\n",
    "    models_data.append({'name': \"PT VGG19 (BCI)\", 'model': model_pl_plus_tl_bci, 'checkpoint': perceptual_plus_texture_loss_bci_checkpoint_path})\n",
    "\n",
    "if enable_pt16_no_res:\n",
    "    model_pl_16_plus_tl_no_res = generator_no_residual(input_shape=train_data.shape[1:], summary=False)\n",
    "    models_data.append({'name': \"PT VGG16 (NR)\", 'model': model_pl_16_plus_tl_no_res, 'checkpoint': perceptual_16_plus_texture_loss_no_res_checkpoint_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_models(test_data_tensors, test_truth_tensors, models_data, test_image_index_to_show, show_input=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the images in higher dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models_single_image(test_data_tensors, test_truth_tensors, models_data, test_image_index_to_show, show_input=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
